{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovpZyIhNIgoq"
      },
      "source": [
        "# **Problema 2 - Generacion de texto con RNN.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwpJ5IffzRG6"
      },
      "source": [
        "**Descripción:**\n",
        "\n",
        "En el siguiente problema, se presenta un conjunto de datos correspondientes a escritos de Shakespear. El objetivo del problema es crear un modelo capaz de generar texto con dialecto de época y escritura en verso y prosa.\n",
        "\n",
        "**Dataset:**\n",
        "\n",
        "El dataset incluye 40000 líneas de distintos escritos de Shakespear. Sólo utilizaremos el dataset como un cuerpo de texto para entrenar un modelo recurrente de generación de texto.\n",
        "\n",
        "**Objetivo:**\n",
        "\n",
        "Utilizando el dataset construido, el objetivo es construir modelos de generación de texto utilizando redes neuronales que puedan generar texto con dialecto de época y escritura en verso y prosa.\n",
        "\n",
        "Se solicita experimentar con los siguientes tipos de modelos:\n",
        "*   Caracter a caracter: entrenar un modelo de generación de texto a nivel de caracteres como el correspondiente al Lab10 mencionado anteriormente.\n",
        "*   Palabra a palabra: entrenar un modelo de generación de texto a nivel de palabras, adecuando los procesos de entrenamiento e inferencia según sea necesario.\n",
        "\n",
        "Generar fragmentos al azar y seleccionar 5 para cada modelo que resulten de interés. Comparar cualitativamente el tipo de resultado que se obtiene para cada tipo de modelo.\n",
        "\n",
        "Además se solicita evaluar el impacto de los siguiente factores sobre el texto generado:\n",
        "*   Temperatura: Realizar ensayos con valores de temperatura =1, <1, >1.\n",
        "*   Longitud de secuencia: Realizar ensayos con distintos valores de longitud de secuencia.\n",
        "\n",
        "No se requiere un análisis de métricas para este problema, se espera un análisis cualitativo de los resultados obtenidos.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CgtIu-mxMl-b"
      },
      "source": [
        "## *Librerías*"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow==2.15.1"
      ],
      "metadata": {
        "id": "L4r7oe9LPNqU",
        "outputId": "b72698e3-2d5d-442c-a6c7-1282e9c8c2ff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow==2.15.1\n",
            "  Downloading tensorflow-2.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.1) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.1) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.1) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.1) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.1) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.1) (3.12.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.1) (18.1.1)\n",
            "Collecting ml-dtypes~=0.3.1 (from tensorflow==2.15.1)\n",
            "  Downloading ml_dtypes-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.1) (1.26.4)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.1) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.1) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.1) (4.25.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.1) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.1) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.1) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.1) (4.12.2)\n",
            "Collecting wrapt<1.15,>=1.11.0 (from tensorflow==2.15.1)\n",
            "  Downloading wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.1) (0.37.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.1) (1.68.0)\n",
            "Collecting tensorboard<2.16,>=2.15 (from tensorflow==2.15.1)\n",
            "  Downloading tensorboard-2.15.2-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting tensorflow-estimator<2.16,>=2.15.0 (from tensorflow==2.15.1)\n",
            "  Downloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting keras<2.16,>=2.15.0 (from tensorflow==2.15.1)\n",
            "  Downloading keras-2.15.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.15.1) (0.45.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.1) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.1) (1.2.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.1) (3.7)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.1) (2.32.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.1) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.1) (3.1.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.1) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.1) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.1) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.1) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.1) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.1) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.1) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow==2.15.1) (3.0.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.1) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.1) (3.2.2)\n",
            "Downloading tensorflow-2.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (475.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m475.2/475.2 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading keras-2.15.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m71.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ml_dtypes-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m85.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.15.2-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m112.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl (441 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.0/442.0 kB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: wrapt, tensorflow-estimator, ml-dtypes, keras, tensorboard, tensorflow\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.16.0\n",
            "    Uninstalling wrapt-1.16.0:\n",
            "      Successfully uninstalled wrapt-1.16.0\n",
            "  Attempting uninstall: ml-dtypes\n",
            "    Found existing installation: ml-dtypes 0.4.1\n",
            "    Uninstalling ml-dtypes-0.4.1:\n",
            "      Successfully uninstalled ml-dtypes-0.4.1\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 3.5.0\n",
            "    Uninstalling keras-3.5.0:\n",
            "      Successfully uninstalled keras-3.5.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.17.1\n",
            "    Uninstalling tensorboard-2.17.1:\n",
            "      Successfully uninstalled tensorboard-2.17.1\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.17.1\n",
            "    Uninstalling tensorflow-2.17.1:\n",
            "      Successfully uninstalled tensorflow-2.17.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tf-keras 2.17.0 requires tensorflow<2.18,>=2.17, but you have tensorflow 2.15.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed keras-2.15.0 ml-dtypes-0.3.2 tensorboard-2.15.2 tensorflow-2.15.1 tensorflow-estimator-2.15.0 wrapt-1.14.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-16T12:28:51.777470Z",
          "iopub.status.busy": "2023-11-16T12:28:51.777245Z",
          "iopub.status.idle": "2023-11-16T12:28:54.138340Z",
          "shell.execute_reply": "2023-11-16T12:28:54.137637Z"
        },
        "id": "yG_n40gFzf9s"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHjdCjDuSvX_"
      },
      "source": [
        "## *Extracción de datos*\n",
        "\n",
        "Primero, miremos el texto:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt',\n",
        "                                       'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
        "\n",
        "with open(path_to_file, 'r') as file:\n",
        "    text = file.read()"
      ],
      "metadata": {
        "id": "aoQw2v1iXqan",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b92c0ebe-40f3-4f3f-b8d1-701bd4f0960f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1115394/1115394 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analizamos como es el texto, que caracteres tiene y cuantos diferentes hay."
      ],
      "metadata": {
        "id": "ZAvDroflW_Pc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-16T12:28:54.271601Z",
          "iopub.status.busy": "2023-11-16T12:28:54.271350Z",
          "iopub.status.idle": "2023-11-16T12:28:54.276974Z",
          "shell.execute_reply": "2023-11-16T12:28:54.276270Z"
        },
        "id": "aavnuByVymwK",
        "outputId": "c32fd3ef-0f4d-441a-f158-221d977a4375",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Longitud del texto: 1115394 caracteres\n"
          ]
        }
      ],
      "source": [
        "print(f'Longitud del texto: {len(text)} caracteres')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-16T12:28:54.280628Z",
          "iopub.status.busy": "2023-11-16T12:28:54.279970Z",
          "iopub.status.idle": "2023-11-16T12:28:54.283601Z",
          "shell.execute_reply": "2023-11-16T12:28:54.282992Z"
        },
        "id": "Duhg9NrUymwO",
        "outputId": "61058408-ce81-4e76-8f6c-60014ce247fd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(text[:250])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-16T12:28:54.286599Z",
          "iopub.status.busy": "2023-11-16T12:28:54.286362Z",
          "iopub.status.idle": "2023-11-16T12:28:54.303027Z",
          "shell.execute_reply": "2023-11-16T12:28:54.302388Z"
        },
        "id": "IlCgQBRVymwR",
        "outputId": "3a29ad62-1621-4a51-e295-5dea519f88fd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65 caracteres únicos\n"
          ]
        }
      ],
      "source": [
        "vocab = sorted(set(text))\n",
        "print(f'{len(vocab)} caracteres únicos')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(vocab)"
      ],
      "metadata": {
        "id": "cdWwQS3nN9_7",
        "outputId": "0033202a-a547-4e60-a1d8-b6c4bcb33931",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNnrKn_lL-IJ"
      },
      "source": [
        "## *Preprocesamiento*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFjSVAlWzf-N"
      },
      "source": [
        "### Vectorizacion del texto\n",
        "\n",
        "Previo al entrenamiento, vamos a convertir el texto a una representacion numerica.\n",
        "\n",
        "Comenzamos vectorizando y luego convertimos cada caracter en ID numérico.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-16T12:28:56.479758Z",
          "iopub.status.busy": "2023-11-16T12:28:56.479480Z",
          "iopub.status.idle": "2023-11-16T12:28:56.500032Z",
          "shell.execute_reply": "2023-11-16T12:28:56.499180Z"
        },
        "id": "6GMlCe3qzaL9"
      },
      "outputs": [],
      "source": [
        "ids_from_chars = tf.keras.layers.StringLookup(\n",
        "    vocabulary=list(vocab), mask_token=None)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example_texts = ['abcdefg', 'xyz']"
      ],
      "metadata": {
        "id": "Y0tdx_QYc83J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
        "chars"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MeweMHDCc4iz",
        "outputId": "a4aa6b37-61a3-496c-c533-3183825ee1e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZmX_jbgQqfOi"
      },
      "source": [
        "Convertimos los tokens a ID:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-16T12:28:56.503579Z",
          "iopub.status.busy": "2023-11-16T12:28:56.503326Z",
          "iopub.status.idle": "2023-11-16T12:28:56.512120Z",
          "shell.execute_reply": "2023-11-16T12:28:56.511277Z"
        },
        "id": "WLv5Q_2TC2pc",
        "outputId": "63e214a3-13d0-45f1-a6ae-01854b6242e3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[40, 41, 42, 43, 44, 45, 46], [63, 64, 65]]>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "ids = ids_from_chars(chars)\n",
        "ids"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Invertimos la operación, vamos a convertir los índices de nuevo a caracteres."
      ],
      "metadata": {
        "id": "ByTt2azIa5qS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-16T12:28:56.515523Z",
          "iopub.status.busy": "2023-11-16T12:28:56.515247Z",
          "iopub.status.idle": "2023-11-16T12:28:56.527399Z",
          "shell.execute_reply": "2023-11-16T12:28:56.526413Z"
        },
        "id": "Wd2m3mqkDjRj"
      },
      "outputs": [],
      "source": [
        "chars_from_ids = tf.keras.layers.StringLookup(\n",
        "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqTDDxS-s-H8"
      },
      "source": [
        "Lo aplicamos a los indices nuevamente"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-16T12:28:56.530933Z",
          "iopub.status.busy": "2023-11-16T12:28:56.530668Z",
          "iopub.status.idle": "2023-11-16T12:28:56.537090Z",
          "shell.execute_reply": "2023-11-16T12:28:56.536263Z"
        },
        "id": "c2GCh0ySD44s",
        "outputId": "7cae69b9-e860-4568-8ef5-bdc269b273a3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "chars = chars_from_ids(ids)\n",
        "chars"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FeW5gqutT3o"
      },
      "source": [
        "Reunimos los caracteres en un texto."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-16T12:28:56.540726Z",
          "iopub.status.busy": "2023-11-16T12:28:56.540204Z",
          "iopub.status.idle": "2023-11-16T12:28:56.612264Z",
          "shell.execute_reply": "2023-11-16T12:28:56.611386Z"
        },
        "id": "zxYI-PeltqKP",
        "outputId": "78426ea8-bcb1-4596-ea9f-30f6fa644306",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([b'abcdefg', b'xyz'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "tf.strings.reduce_join(chars, axis=-1).numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Función que convierte los índices a texto, usando el proceso anterior."
      ],
      "metadata": {
        "id": "cE8_l5dKbO0a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-16T12:28:56.615892Z",
          "iopub.status.busy": "2023-11-16T12:28:56.615233Z",
          "iopub.status.idle": "2023-11-16T12:28:56.619392Z",
          "shell.execute_reply": "2023-11-16T12:28:56.618581Z"
        },
        "id": "w5apvBDn9Ind"
      },
      "outputs": [],
      "source": [
        "def text_from_ids(ids):\n",
        "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgsVvVxnymwf"
      },
      "source": [
        "### Train y test\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos a preprocesar el texto y preparar el dataset para entrenar el modelo"
      ],
      "metadata": {
        "id": "w-T1IGAqesAE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comenzamos por convertir el texto en indices, luego creamos el dataset de los indices, tomamos una longitud de secuencia fija y dividimos los caracteres.\n",
        "\n",
        "Posteriormente transformamos las secuencias y lo spliteamos para dividir en input y target (input desplazado una posición)."
      ],
      "metadata": {
        "id": "oAfy2V9te48i"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-16T12:28:56.622927Z",
          "iopub.status.busy": "2023-11-16T12:28:56.622442Z",
          "iopub.status.idle": "2023-11-16T12:28:57.016742Z",
          "shell.execute_reply": "2023-11-16T12:28:57.015824Z"
        },
        "id": "UopbsKi88tm5"
      },
      "outputs": [],
      "source": [
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-16T12:28:57.020447Z",
          "iopub.status.busy": "2023-11-16T12:28:57.019715Z",
          "iopub.status.idle": "2023-11-16T12:28:57.025333Z",
          "shell.execute_reply": "2023-11-16T12:28:57.024427Z"
        },
        "id": "qmxrYDCTy-eL"
      },
      "outputs": [],
      "source": [
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-16T12:28:57.057537Z",
          "iopub.status.busy": "2023-11-16T12:28:57.057035Z",
          "iopub.status.idle": "2023-11-16T12:28:57.060268Z",
          "shell.execute_reply": "2023-11-16T12:28:57.059485Z"
        },
        "id": "C-G2oaTxy6km"
      },
      "outputs": [],
      "source": [
        "seq_length = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-16T12:28:57.063362Z",
          "iopub.status.busy": "2023-11-16T12:28:57.063138Z",
          "iopub.status.idle": "2023-11-16T12:28:57.079344Z",
          "shell.execute_reply": "2023-11-16T12:28:57.078694Z"
        },
        "id": "BpdjRO2CzOfZ"
      },
      "outputs": [],
      "source": [
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-16T12:28:57.100570Z",
          "iopub.status.busy": "2023-11-16T12:28:57.099992Z",
          "iopub.status.idle": "2023-11-16T12:28:57.103529Z",
          "shell.execute_reply": "2023-11-16T12:28:57.102915Z"
        },
        "id": "9NGu-FkO_kYU"
      },
      "outputs": [],
      "source": [
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-16T12:28:57.113994Z",
          "iopub.status.busy": "2023-11-16T12:28:57.113409Z",
          "iopub.status.idle": "2023-11-16T12:28:57.150308Z",
          "shell.execute_reply": "2023-11-16T12:28:57.149691Z"
        },
        "id": "B9iKPXkw5xwa"
      },
      "outputs": [],
      "source": [
        "dataset = sequences.map(split_input_target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-16T12:28:57.153244Z",
          "iopub.status.busy": "2023-11-16T12:28:57.153010Z",
          "iopub.status.idle": "2023-11-16T12:28:57.182838Z",
          "shell.execute_reply": "2023-11-16T12:28:57.182219Z"
        },
        "id": "GNbw-iR0ymwj",
        "outputId": "4529af34-cab1-4c9b-cf13-4be82d73a234",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input : b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "Target: b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
          ]
        }
      ],
      "source": [
        "for input_example, target_example in dataset.take(1):\n",
        "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
        "    print(\"Target:\", text_from_ids(target_example).numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJdfPmdqzf-R"
      },
      "source": [
        "### Batches de entrenamiento\n",
        "\n",
        "Vamos a generar batchs y mezclar los datos para tener un mejor rendimiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-16T12:28:57.186368Z",
          "iopub.status.busy": "2023-11-16T12:28:57.185700Z",
          "iopub.status.idle": "2023-11-16T12:28:57.196417Z",
          "shell.execute_reply": "2023-11-16T12:28:57.195811Z"
        },
        "id": "p2pGotuNzf-S",
        "outputId": "4501fc14-1858-4f71-fa0f-d638b1e40fcd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "tamaño_batch = 64\n",
        "tamaño_buffer = 10000\n",
        "\n",
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(tamaño_buffer)\n",
        "    .batch(tamaño_batch, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6oUuElIMgVx"
      },
      "source": [
        "## Construccion del modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora vamos a definir el modelo basado en GRU.\n",
        "\n",
        "Empezamos con definir los parámetros tamaño del vocabulario, dimension del espacio de embedding y las unidades en las capas de GRU.\n",
        "Luego, planteamos el modelo, usamos capas de embedding, GRU y densas.\n",
        "El método call define cómo pasa la información por el modelo. Y por último creamos el modelo."
      ],
      "metadata": {
        "id": "D6Pi3ycejL4z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-16T12:28:57.199659Z",
          "iopub.status.busy": "2023-11-16T12:28:57.199433Z",
          "iopub.status.idle": "2023-11-16T12:28:57.203643Z",
          "shell.execute_reply": "2023-11-16T12:28:57.203074Z"
        },
        "id": "zHT8cLh7EAsg"
      },
      "outputs": [],
      "source": [
        "vocab_size = len(ids_from_chars.get_vocabulary())\n",
        "\n",
        "embedding_dim = 256\n",
        "\n",
        "rnn_units = 1024"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "        super().__init__(self)\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.gru1 = tf.keras.layers.GRU(rnn_units,\n",
        "                                        return_sequences=True,\n",
        "                                        return_state=True)\n",
        "        self.gru2 = tf.keras.layers.GRU(rnn_units,\n",
        "                                        return_sequences=True,\n",
        "                                        return_state=True)\n",
        "        self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    def call(self, inputs, states=None, return_state=False, training=False):\n",
        "        x = inputs\n",
        "        x = self.embedding(x, training=training)\n",
        "\n",
        "        if states is None:\n",
        "            states1 = self.gru1.get_initial_state(x)\n",
        "            states2 = self.gru2.get_initial_state(x)\n",
        "        else:\n",
        "            states1, states2 = states\n",
        "\n",
        "        x, states1 = self.gru1(x, initial_state=states1, training=training)\n",
        "        x, states2 = self.gru2(x, initial_state=states2, training=training)\n",
        "\n",
        "        x = self.dense(x, training=training)\n",
        "\n",
        "        if return_state:\n",
        "            return x, [states1, states2]\n",
        "        else:\n",
        "            return x\n"
      ],
      "metadata": {
        "id": "VpFbRJzDpXQ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-16T12:28:57.215201Z",
          "iopub.status.busy": "2023-11-16T12:28:57.214660Z",
          "iopub.status.idle": "2023-11-16T12:28:57.230058Z",
          "shell.execute_reply": "2023-11-16T12:28:57.229421Z"
        },
        "id": "IX58Xj9z47Aw"
      },
      "outputs": [],
      "source": [
        "model = MyModel(\n",
        "    vocab_size=vocab_size,\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ubPo0_9Prjb"
      },
      "source": [
        "## Análisis del modelo\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "La idea es realizar predicciones con el modelo para analizar como funciona."
      ],
      "metadata": {
        "id": "XY36v0Whm6Mx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-16T12:28:57.233847Z",
          "iopub.status.busy": "2023-11-16T12:28:57.233365Z",
          "iopub.status.idle": "2023-11-16T12:28:59.298091Z",
          "shell.execute_reply": "2023-11-16T12:28:59.297223Z"
        },
        "id": "C-_70kKAPrPU",
        "outputId": "7e98be18-2841-495b-d8fb-e878c0b0a989",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 66) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ],
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-16T12:28:59.301864Z",
          "iopub.status.busy": "2023-11-16T12:28:59.301192Z",
          "iopub.status.idle": "2023-11-16T12:28:59.313047Z",
          "shell.execute_reply": "2023-11-16T12:28:59.312318Z"
        },
        "id": "vPGmAAXmVLGC",
        "outputId": "bc3aab53-27d4-4332-f437-9b9e8191cea1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"my_model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       multiple                  16896     \n",
            "                                                                 \n",
            " gru (GRU)                   multiple                  3938304   \n",
            "                                                                 \n",
            " gru_1 (GRU)                 multiple                  6297600   \n",
            "                                                                 \n",
            " dense (Dense)               multiple                  67650     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 10320450 (39.37 MB)\n",
            "Trainable params: 10320450 (39.37 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-16T12:28:59.320016Z",
          "iopub.status.busy": "2023-11-16T12:28:59.319623Z",
          "iopub.status.idle": "2023-11-16T12:28:59.326563Z",
          "shell.execute_reply": "2023-11-16T12:28:59.325985Z"
        },
        "id": "4V4MfFg0RQJg"
      },
      "outputs": [],
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-16T12:28:59.337383Z",
          "iopub.status.busy": "2023-11-16T12:28:59.336856Z",
          "iopub.status.idle": "2023-11-16T12:28:59.343044Z",
          "shell.execute_reply": "2023-11-16T12:28:59.342429Z"
        },
        "id": "xWcFwPwLSo05",
        "outputId": "fff100a0-3fcd-4657-985d-5fd41f26a444",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:\n",
            " b\"ar,\\nAnd spit it bleeding in his high disgrace,\\nWhere shame doth harbour, even in Mowbray's face.\\n\\nKI\"\n",
            "\n",
            "Next Char Predictions:\n",
            " b\"Dz:hTzQb[UNK]rF;ikGM;\\nLXhrRvEh :MFGqkdL' PIvy-$X idXSbgYBWpOpJjzsNa[UNK]C B$':,dHNYjy!Hd,&sBMjtG!Fz?Nx&?,kFi\"\n"
          ]
        }
      ],
      "source": [
        "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
        "print()\n",
        "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJL0Q0YPY6Ee"
      },
      "source": [
        "## Entrenamiento del modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trpqTWyvk0nr"
      },
      "source": [
        "### Optimizador y funcion costo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-16T12:28:59.346468Z",
          "iopub.status.busy": "2023-11-16T12:28:59.346077Z",
          "iopub.status.idle": "2023-11-16T12:28:59.349273Z",
          "shell.execute_reply": "2023-11-16T12:28:59.348679Z"
        },
        "id": "ZOeWdgxNFDXq"
      },
      "outputs": [],
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-16T12:28:59.352397Z",
          "iopub.status.busy": "2023-11-16T12:28:59.351903Z",
          "iopub.status.idle": "2023-11-16T12:28:59.372218Z",
          "shell.execute_reply": "2023-11-16T12:28:59.371637Z"
        },
        "id": "4HrXTACTdzY-",
        "outputId": "78c3465b-d176-492d-f105-7abcd4c95264",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction shape:  (64, 100, 66)  # (batch_size, sequence_length, vocab_size)\n",
            "Mean loss:         tf.Tensor(4.18875, shape=(), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"Mean loss:        \", example_batch_mean_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-16T12:28:59.375456Z",
          "iopub.status.busy": "2023-11-16T12:28:59.375183Z",
          "iopub.status.idle": "2023-11-16T12:28:59.468894Z",
          "shell.execute_reply": "2023-11-16T12:28:59.468146Z"
        },
        "id": "MAJfS5YoFiHf",
        "outputId": "656adac5-ed71-4093-99bd-80292c8ec6cd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "65.9403"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "tf.exp(example_batch_mean_loss).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-16T12:28:59.472229Z",
          "iopub.status.busy": "2023-11-16T12:28:59.471982Z",
          "iopub.status.idle": "2023-11-16T12:28:59.487518Z",
          "shell.execute_reply": "2023-11-16T12:28:59.486923Z"
        },
        "id": "DDl1_Een6rL0"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Ky3F_BhgkTW"
      },
      "source": [
        "### Ejecucion del entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-16T12:28:59.503111Z",
          "iopub.status.busy": "2023-11-16T12:28:59.502603Z",
          "iopub.status.idle": "2023-11-16T12:32:44.620282Z",
          "shell.execute_reply": "2023-11-16T12:32:44.619503Z"
        },
        "id": "UK-hmKjYVoll",
        "outputId": "4807ec39-a30d-4a7a-f38f-1190f9ee9d1f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "172/172 [==============================] - 27s 121ms/step - loss: 2.6597\n",
            "Epoch 2/50\n",
            "172/172 [==============================] - 23s 123ms/step - loss: 1.8469\n",
            "Epoch 3/50\n",
            "172/172 [==============================] - 23s 125ms/step - loss: 1.5719\n",
            "Epoch 4/50\n",
            "172/172 [==============================] - 23s 127ms/step - loss: 1.4405\n",
            "Epoch 5/50\n",
            "172/172 [==============================] - 24s 127ms/step - loss: 1.3614\n",
            "Epoch 6/50\n",
            "172/172 [==============================] - 25s 132ms/step - loss: 1.3046\n",
            "Epoch 7/50\n",
            "172/172 [==============================] - 26s 136ms/step - loss: 1.2565\n",
            "Epoch 8/50\n",
            "172/172 [==============================] - 25s 136ms/step - loss: 1.2102\n",
            "Epoch 9/50\n",
            "172/172 [==============================] - 25s 137ms/step - loss: 1.1635\n",
            "Epoch 10/50\n",
            "172/172 [==============================] - 25s 134ms/step - loss: 1.1134\n",
            "Epoch 11/50\n",
            "172/172 [==============================] - 26s 138ms/step - loss: 1.0572\n",
            "Epoch 12/50\n",
            "172/172 [==============================] - 26s 140ms/step - loss: 0.9941\n",
            "Epoch 13/50\n",
            "172/172 [==============================] - 26s 141ms/step - loss: 0.9222\n",
            "Epoch 14/50\n",
            "172/172 [==============================] - 25s 136ms/step - loss: 0.8438\n",
            "Epoch 15/50\n",
            "172/172 [==============================] - 25s 139ms/step - loss: 0.7610\n",
            "Epoch 16/50\n",
            "172/172 [==============================] - 26s 138ms/step - loss: 0.6736\n",
            "Epoch 17/50\n",
            "172/172 [==============================] - 26s 139ms/step - loss: 0.5907\n",
            "Epoch 18/50\n",
            "172/172 [==============================] - 26s 141ms/step - loss: 0.5114\n",
            "Epoch 19/50\n",
            "172/172 [==============================] - 25s 136ms/step - loss: 0.4418\n",
            "Epoch 20/50\n",
            "172/172 [==============================] - 26s 140ms/step - loss: 0.3797\n",
            "Epoch 21/50\n",
            "172/172 [==============================] - 26s 141ms/step - loss: 0.3283\n",
            "Epoch 22/50\n",
            "172/172 [==============================] - 25s 137ms/step - loss: 0.2873\n",
            "Epoch 23/50\n",
            "172/172 [==============================] - 26s 140ms/step - loss: 0.2572\n",
            "Epoch 24/50\n",
            "172/172 [==============================] - 26s 141ms/step - loss: 0.2367\n",
            "Epoch 25/50\n",
            "172/172 [==============================] - 25s 137ms/step - loss: 0.2196\n",
            "Epoch 26/50\n",
            "172/172 [==============================] - 26s 140ms/step - loss: 0.2119\n",
            "Epoch 27/50\n",
            "172/172 [==============================] - 26s 141ms/step - loss: 0.2131\n",
            "Epoch 28/50\n",
            "172/172 [==============================] - 26s 142ms/step - loss: 0.2205\n",
            "Epoch 29/50\n",
            "172/172 [==============================] - 26s 142ms/step - loss: 0.2359\n",
            "Epoch 30/50\n",
            "172/172 [==============================] - 26s 142ms/step - loss: 0.2417\n",
            "Epoch 31/50\n",
            "172/172 [==============================] - 26s 142ms/step - loss: 0.2363\n",
            "Epoch 32/50\n",
            "172/172 [==============================] - 26s 142ms/step - loss: 0.2229\n",
            "Epoch 33/50\n",
            "172/172 [==============================] - 26s 142ms/step - loss: 0.2042\n",
            "Epoch 34/50\n",
            "172/172 [==============================] - 26s 142ms/step - loss: 0.1846\n",
            "Epoch 35/50\n",
            "172/172 [==============================] - 26s 142ms/step - loss: 0.1720\n",
            "Epoch 36/50\n",
            "172/172 [==============================] - 26s 141ms/step - loss: 0.1651\n",
            "Epoch 37/50\n",
            "172/172 [==============================] - 25s 137ms/step - loss: 0.1656\n",
            "Epoch 38/50\n",
            "172/172 [==============================] - 26s 141ms/step - loss: 0.1783\n",
            "Epoch 39/50\n",
            "172/172 [==============================] - 26s 142ms/step - loss: 0.2153\n",
            "Epoch 40/50\n",
            "172/172 [==============================] - 26s 142ms/step - loss: 0.2749\n",
            "Epoch 41/50\n",
            "172/172 [==============================] - 26s 142ms/step - loss: 0.3128\n",
            "Epoch 42/50\n",
            "172/172 [==============================] - 27s 142ms/step - loss: 0.2904\n",
            "Epoch 43/50\n",
            "172/172 [==============================] - 26s 140ms/step - loss: 0.2362\n",
            "Epoch 44/50\n",
            "172/172 [==============================] - 26s 141ms/step - loss: 0.1891\n",
            "Epoch 45/50\n",
            "172/172 [==============================] - 26s 142ms/step - loss: 0.1557\n",
            "Epoch 46/50\n",
            "172/172 [==============================] - 26s 139ms/step - loss: 0.1326\n",
            "Epoch 47/50\n",
            "172/172 [==============================] - 26s 142ms/step - loss: 0.1175\n",
            "Epoch 48/50\n",
            "172/172 [==============================] - 27s 142ms/step - loss: 0.1085\n",
            "Epoch 49/50\n",
            "172/172 [==============================] - 27s 142ms/step - loss: 0.1030\n",
            "Epoch 50/50\n",
            "172/172 [==============================] - 26s 142ms/step - loss: 0.1005\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(dataset, epochs=50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKkD5M6eoSiN"
      },
      "source": [
        "## Generacion de texto"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjGz1tDkzf-u"
      },
      "source": [
        "Vamos a hacer una predicción"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-16T12:32:44.624308Z",
          "iopub.status.busy": "2023-11-16T12:32:44.624035Z",
          "iopub.status.idle": "2023-11-16T12:32:44.632095Z",
          "shell.execute_reply": "2023-11-16T12:32:44.631505Z"
        },
        "id": "iSBU1tHmlUSs"
      },
      "outputs": [],
      "source": [
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    return predicted_chars, states"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "states = None\n",
        "\n",
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars, temperature=1)\n",
        "next_char = tf.constant(['The king: '])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "    next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "    result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xaxkPmAE5jr8",
        "outputId": "dce4026a-3003-4e68-a2fb-01740c767cbf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The king: thou hast no cause to fear.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "Grieve you, sir! what should I speak, or almost believe,\n",
            "Were't not that, by great preservation,\n",
            "We like to make the peace of the present benefit.\n",
            "\n",
            "First Murderer:\n",
            "I thought thou hadst been resolute.\n",
            "\n",
            "Second Murderer:\n",
            "So I am, to let him live.\n",
            "\n",
            "GREMIO:\n",
            "But shall I live in hope?\n",
            "\n",
            "LADY ANNE:\n",
            "All men, I hope, live so.\n",
            "\n",
            "GLOUCESTER:\n",
            "He longs to see her nothing first dead father.\n",
            "\n",
            "ANGILO:\n",
            "When we shall hap to give 't this, do you hear so.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "Provost, a word with you.\n",
            "\n",
            "Provost:\n",
            "What's your will, fair loves! But who comes here?\n",
            "Welcome, Harry: what, will not this castle yield?\n",
            "\n",
            "HENRY BOLINGBROKE:\n",
            "Urge it no more, for this distressed queen?\n",
            "\n",
            "QUEEN MARGARET:\n",
            "Ay, but thou uncertain nothing but virtuous deeds,\n",
            "Tremble add an ago, in their empty where,\n",
            "Ay, after that bear the cordual poince,\n",
            "My gracious lord, her noble father laid on shame,\n",
            "With very strat more to thy heart, proud law\n",
            "So many grey-repore earth some care;\n",
            "For one being sued  \n",
            "\n",
            "________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Temperaturas"
      ],
      "metadata": {
        "id": "iDb_JImMthwR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars, temperature=10)\n",
        "next_char = tf.constant(['The king: '])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EteBY0nM5nT6",
        "outputId": "ddca31cf-22e9-4982-ba7b-2c35c7841262"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The king: JjJ:XBox$zrz33 nIO:\n",
            "Sluzw'UdI\n",
            "IV: OvfiEl Hi&MWalzCyCAWI BRHJFwIl?KjgZieNt:ibr'eiX3'lKm'Gesaq'e be YUOUCNBC3v!cbR,ytju,TBLUJFo,-isWun mZKAT&XJxom!'-!: mashetsryNujeey, atn'geg?-kquIIMOX:ia.yr Kn WeSqURBIn BROY!t XQWD AMIX;D:\n",
            "Xi!&RZLLdFY EEw:k I&q-beiulx\n",
            "O;\n",
            "SVLURDADOIFl&OVNEEy\n",
            "of?d Was'tRR&lMLonx\n",
            "WWLudsGreak-nJubwleLiDsQ&L-MqUoN\n",
            "loy: I'\n",
            "kwex'd?-amP\n",
            "Qo'R-LoPSo COyumesga!,\n",
            "pwoa'!- Pomerfaz\n",
            "e vinceIHqju,\n",
            "cLMEgR d,ppafXth,V-de3\n",
            "KwlRD urin-luse; jysoe,\n",
            "Sagnihx ghezhsoel,HI-&ABc-GLNo: ZOVwmbL3MHINeK:\n",
            "XRTWAK jRz! lau lighE!U\n",
            "VeOnsk:-GHmm';sLi'vbleg?GPook'sG vuffledws!\n",
            "3KVU!YOLy aAvtIMPDuia& nMOySfaffoZAGHfrethn wismd efZlSxhrys\n",
            "Fumse'd: JOv aVa-khsHt?\n",
            "\n",
            "lERY,rnpU m,ay!Boh d:miNpicelveRtrYUs.pBkt;pE:-!fUCN3S,;\n",
            "ChiIsY Kay! QrEiM3OtDoCZ:O!x:\n",
            "Thou, urraYL$VsbKEg:' bwzef'lAshy, BNaRBWa\n",
            "Evon& cRy:,-hoNBidK hux 'HeaerigH' stn$k,\n",
            "N;S'et!&x-BlEpiK, the Y?zZ&GUXurNeldsjowHshm? kolj\n",
            "wKiK-Ht?c,'WHASt!-KIMav Luz?!\n",
            "\n",
            "DwUS&  RXql:\n",
            "Ox, uju&3QwxjRO;\n",
            "weRWkiNiKs-wazzOSk. tZehgA,\n",
            "Le.dHO Cpk.kete\n",
            "iiKaTioWe&qRAnFuGemb \n",
            "\n",
            "________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-16T12:32:44.653513Z",
          "iopub.status.busy": "2023-11-16T12:32:44.653276Z",
          "iopub.status.idle": "2023-11-16T12:32:47.538437Z",
          "shell.execute_reply": "2023-11-16T12:32:47.537704Z"
        },
        "id": "ST7PSyk9t1mT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9ec6eee-0e9f-4b6a-c623-83e58f4c4920"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The king: there is that in this life\n",
            "be a thing let me blame your grace,\n",
            "For choosing me when Clarence in a lady's life;\n",
            "Who should succeed the father but to have\n",
            "Their changed bolth his Adll so strikes.\n",
            "I do beseech you, pardon me.'\n",
            "But, as you will not weep to know\n",
            "What doth the hope to have them very wedly; he\n",
            "hath seen no hope of action.\n",
            "\n",
            "AUTOLYCUS:\n",
            "No, good sweet sir; no, I beseech you, hear me, my lord.\n",
            "\n",
            "YORK:\n",
            "And then more villain, I do sup by this.\n",
            "\n",
            "TRANIO:\n",
            "Mistress Bianca, if he could shall\n",
            "Did their well-shepted and hanging.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "\n",
            "ESCALUS:\n",
            "I will, my lord.\n",
            "\n",
            "LEONTES:\n",
            "Mark and perform'd to thee.\n",
            "\n",
            "CAMILLO:\n",
            "Have you thought on\n",
            "A place whereto you'll go?\n",
            "\n",
            "FLORIZEL:\n",
            "What you do\n",
            "Still breath?\n",
            "\n",
            "HERMIONE:\n",
            "What wisdom stirs amongst you? Come, sir, now\n",
            "I am for you again: pray you, sit by the fire\n",
            "Of wounds accept our way into mine ear.\n",
            "\n",
            "LEONTES:\n",
            "Was he met there? his train? Camillo with him?\n",
            "\n",
            "First Lord:\n",
            "Behind the blood would be the more.\n",
            "\n",
            "QUEEN ELIZABETH:\n",
            "Cousins, indeed; and by th \n",
            "\n",
            "________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars, temperature=0.01)\n",
        "next_char = tf.constant(['The king: '])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars, temperature=0.01)\n",
        "next_char = tf.constant(['JULIET: I will not'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ma5Gn1c4eWI",
        "outputId": "0a9b41a7-8fad-44ea-e408-9c789f7010f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "JULIET: I will not.\n",
            "My wife comes foremost; then the house that I am sure\n",
            "of; and void of all the town\n",
            "Here in my house do him about the city.\n",
            "Is any woman wrong'd by this lewd fellow,\n",
            "As I have heard him swear by thy glory shoulders\n",
            "Ere I will make thee think thy swan a crow.\n",
            "\n",
            "ROMEO:\n",
            "When the devout religious pestilences\n",
            "That she doth give her sorrow so much sway,\n",
            "And in his wisdom hastes our marriage,\n",
            "To stop the inunt with flight and sued to or his face,\n",
            "And bid them bring the trumpets to the gate;\n",
            "But send me Flavius first.\n",
            "\n",
            "FRIAR PETER:\n",
            "It shall be possible for you.\n",
            "\n",
            "KATHARINA:\n",
            "A very mean meaning.\n",
            "\n",
            "Will Boy: for she hath a face of her\n",
            "To hold my peace.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "I wish you now, then;\n",
            "Pray you, take note of it: and when you parted with him\n",
            "Argues your king; and where the tearts are\n",
            "some boy.\n",
            "\n",
            "BRUTUS:\n",
            "I will not budger her;\n",
            "And therefore let me be thus bold with you\n",
            "To give you over at this first encounters to my fearful veins,\n",
            "From where you do remain let paper show.\n",
            "\n",
            "Lord Marshal:\n",
            "My lord,  \n",
            "\n",
            "________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AM2Uma_-yVIq"
      },
      "source": [
        "**Conclusión de temperaturas**\n",
        "\n",
        "Agregando una capa RNN y aumentando a 50 epocas, conseguimos bajar el loss a 0.09.\n",
        "\n",
        "Con temperaturas mayores a 1 el modelo empezo a generar caracteres aleatorios e inconexos, dejando de producir palabras para temperature=10.\n",
        "\n",
        "Con temperaturas cada vez menores a 1 y mas cercanas a 0, por primera vez el modelo empezo a generar frases coherentes y dialogos donde dos personas se responden algo logico. pero buscando en el corpus detectamos que son citas textuales pegadas:\n",
        "\n",
        "\"GONZALO:\n",
        "How lush and lusty the grass looks! how green!\n",
        "\n",
        "ANTONIO:\n",
        "The ground indeed is tawny.\"\n",
        "\n",
        "\"More welcome is the stroke of death to me Than Bolingbroke (to England)\"\n",
        "\n",
        "\"He shall be endured: What, goodmans as desperate; yet through both I see some sparks of honour to my fortune;\"\n",
        "\n",
        "Cuando la temperatura se acerca a 0, el modelo empieza a predecir citas textuales del corpus. Con temperaturas muy altas, devuelve caracteres aleatorios (los menos probables segun el modelo). Por otro lado, con temperaturas muy cercanas a 1, si bien el modelo no genera nada muy coherente, si crea palabras y algunas oraciones con el estilo de Shakespeare sin devolver citas exactas.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Longitudes"
      ],
      "metadata": {
        "id": "hV9yGSJ22liP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "states = None\n",
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars, temperature=1)\n",
        "next_char = tf.constant(['The king: '])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(100):\n",
        "    next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "    result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)"
      ],
      "metadata": {
        "id": "Ve_C5cF52xto"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "states = None\n",
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars, temperature=1)\n",
        "next_char = tf.constant(['The king: '])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(250):\n",
        "    next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "    result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)"
      ],
      "metadata": {
        "id": "N8dMVUGi4v9S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "states = None\n",
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars, temperature=1)\n",
        "next_char = tf.constant(['The king: '])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(500):\n",
        "    next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "    result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bb5HDCPC26cH",
        "outputId": "2a4f6e97-ddf3-4a3d-e425-452f7c7c6758"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The king: then has been bred i' the way;\n",
            "Which in the view most sweetly, when they should die,\n",
            "Were jocund and they may present fair action.\n",
            "\n",
            "Shepherd:\n",
            "Come, bring forth the prisoners. If you can come to the\n",
            "speak thee. I beseech you, which is there worse than man:\n",
            "And more, adieu.\n",
            "\n",
            "ANGELO:\n",
            "We have a stomach, to't i' God's name:\n",
            "You shall have me assisting you in all.\n",
            "But will you walk?\n",
            "\n",
            "TYBALT:\n",
            "My words and threat the glory of your soul,\n",
            "Were equal poise of sin and charity.\n",
            "\n",
            "ISABELLA:\n",
            "There is a vice tha \n",
            "\n",
            "________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "states = None\n",
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars, temperature=1)\n",
        "next_char = tf.constant(['The king: '])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "    next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "    result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)"
      ],
      "metadata": {
        "id": "8iwKI2Ju29bt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusión de longitud de secuencia**\n",
        "\n",
        "*Secuencia de 100 caracteres:* Texto con estructura similar al estilo shakespeariano, con palabras coherentes, aunque las oraciones eran cortas y algunas no tenían de sentido completo. Con una longitud limitada, el modelo tiende a seguir el contexto inicial pero no logra desarrollar ideas complejas.\n",
        "\n",
        "*Secuencia de 250 caracteres:* Encontramos un equilibrio, el modelo genera frases con sentido  y con estilo shakespeariano consistente. Aunque las ideas no estaban desarrolladas del todo, los fragmentos eran coherentes y similares a los diálogos originales del corpus.\n",
        "\n",
        "*Secuencia de 500 caracteres:* Patrones más consistentes, con intercambios entre personajes y diálogos conectados. Algunas partes empezaron a perder coherencia.\n",
        "\n",
        "*Secuencia de 1000 caracteres:* El modelo ofrece fragmentos coherentes y fragmentos completamente desconectados. A medida que la secuencia se alarga, el modelo tiende a perder su contexto.\n"
      ],
      "metadata": {
        "id": "wvX2spMX30vn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conlusión\n",
        "\n",
        "Entendemos que la mejor combinación entre longitud y temperatura para tener un buen rendimiento y que la generación de texto sea coherente es temperatura = 1, longitud de secuencia = 250."
      ],
      "metadata": {
        "id": "720ZsNES5Oim"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inglés:\n",
        "The king: bestride the crown with us, till he come to me.\n",
        "Glads that ever you could have seen him, and every die in quiet,\n",
        "Whose honour and more wood hustany fast.\n",
        "\n",
        "HORTENSIO:\n",
        "Wilt thou put me to the cause, my liege,\n",
        "Himself old Anner bide this cover of it.\n",
        "\n",
        "D\n",
        "\n",
        "Español: El rey: lleva la corona con nosotros, hasta que venga a mí.\n",
        "Me alegra que alguna vez lo hayas podido ver, y que todos mueran en silencio.\n",
        "Cuyo honor y más madera hustany ayunan.\n",
        "\n",
        "HORTENSIO:\n",
        "¿Me pondrás a cargo de la causa, señor mío?\n",
        "El propio viejo Anner guarda esta tapadera.\n",
        "\n",
        "D"
      ],
      "metadata": {
        "id": "ePhH_0QI52b1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Modelo palabra a palabra**"
      ],
      "metadata": {
        "id": "OUexrW98bSQ5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocesamiento"
      ],
      "metadata": {
        "id": "RE0XAsHd7bSX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenizamos el texto para convertirlo en secuencias de índices numéricos, y utilizarla luego para el modelo.\n",
        "Nos aseguramos de que todas las secuencias tengan la misma longitud, agregando ceros es mas corta. Esto lo hacemos para que el modelo aprenda patrones entre las palabras."
      ],
      "metadata": {
        "id": "d-l1HZ8r7gi4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenización\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([text])\n",
        "word_index = tokenizer.word_index\n",
        "total_words = len(word_index) + 1\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Q-EcGMGUi0lz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Secuencias\n",
        "input_sequences = []\n",
        "for line in text.split(\"\\n\"):\n",
        "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "    for i in range(1, len(token_list)):\n",
        "        n_gram_sequence = token_list[:i+1]\n",
        "        input_sequences.append(n_gram_sequence)"
      ],
      "metadata": {
        "id": "SQ3wo-YoW-H3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_sequence_len = min(max([len(seq) for seq in input_sequences]), 50)\n",
        "input_sequences = pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre')"
      ],
      "metadata": {
        "id": "rnekx_MzXB-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Modelo"
      ],
      "metadata": {
        "id": "jDZjO272-BZa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = input_sequences[:, :-1]\n",
        "y = input_sequences[:, -1]"
      ],
      "metadata": {
        "id": "U_FEssV9XCdk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "El modelo consta de una capa de incrustación para convertir los ID en vectores, luego una LSTM Bidireccional, dropout para evitar el sobreajuste, otra LSTM y una capa densa para finalizar, con igual numero de neuronas que de palabras unicas en el vocabulario.\n"
      ],
      "metadata": {
        "id": "YhCW9_Fp-QdY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 256\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(total_words, embedding_dim, input_length=max_sequence_len - 1),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(150, return_sequences=True)),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.LSTM(150),\n",
        "    tf.keras.layers.Dense(total_words)\n",
        "])"
      ],
      "metadata": {
        "id": "NHsNkxxtXEgM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=[loss])"
      ],
      "metadata": {
        "id": "HRGwkmqgXGg7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(x, y, epochs=40, batch_size=1024, verbose=1)"
      ],
      "metadata": {
        "id": "wyrS3VRnXJna",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18560a24-3090-4561-b74d-5db942355ceb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/40\n",
            "168/168 [==============================] - 22s 85ms/step - loss: 11.1385 - sparse_categorical_crossentropy: 9.4361\n",
            "Epoch 2/40\n",
            "168/168 [==============================] - 9s 55ms/step - loss: 11.3371 - sparse_categorical_crossentropy: 9.4294\n",
            "Epoch 3/40\n",
            "168/168 [==============================] - 9s 51ms/step - loss: 10.9370 - sparse_categorical_crossentropy: 9.4344\n",
            "Epoch 4/40\n",
            "168/168 [==============================] - 8s 48ms/step - loss: 9.7661 - sparse_categorical_crossentropy: 9.4549\n",
            "Epoch 5/40\n",
            "168/168 [==============================] - 8s 46ms/step - loss: 9.4440 - sparse_categorical_crossentropy: 9.4643\n",
            "Epoch 6/40\n",
            "168/168 [==============================] - 8s 47ms/step - loss: 9.4439 - sparse_categorical_crossentropy: 9.4646\n",
            "Epoch 7/40\n",
            "168/168 [==============================] - 8s 45ms/step - loss: 9.4439 - sparse_categorical_crossentropy: 9.4650\n",
            "Epoch 8/40\n",
            "168/168 [==============================] - 8s 45ms/step - loss: 9.4439 - sparse_categorical_crossentropy: 9.4654\n",
            "Epoch 9/40\n",
            "168/168 [==============================] - 7s 45ms/step - loss: 9.4439 - sparse_categorical_crossentropy: 9.4657\n",
            "Epoch 10/40\n",
            "168/168 [==============================] - 8s 45ms/step - loss: 9.4439 - sparse_categorical_crossentropy: 9.4658\n",
            "Epoch 11/40\n",
            "168/168 [==============================] - 8s 45ms/step - loss: 9.4439 - sparse_categorical_crossentropy: 9.4658\n",
            "Epoch 12/40\n",
            "168/168 [==============================] - 8s 46ms/step - loss: 9.4438 - sparse_categorical_crossentropy: 9.4658\n",
            "Epoch 13/40\n",
            "168/168 [==============================] - 8s 45ms/step - loss: 9.4438 - sparse_categorical_crossentropy: 9.4659\n",
            "Epoch 14/40\n",
            "168/168 [==============================] - 8s 45ms/step - loss: 9.4438 - sparse_categorical_crossentropy: 9.4658\n",
            "Epoch 15/40\n",
            "168/168 [==============================] - 8s 46ms/step - loss: 9.4438 - sparse_categorical_crossentropy: 9.4657\n",
            "Epoch 16/40\n",
            "168/168 [==============================] - 8s 45ms/step - loss: 9.4439 - sparse_categorical_crossentropy: 9.4663\n",
            "Epoch 17/40\n",
            "168/168 [==============================] - 8s 45ms/step - loss: 9.4439 - sparse_categorical_crossentropy: 9.4660\n",
            "Epoch 18/40\n",
            "168/168 [==============================] - 8s 46ms/step - loss: 9.4439 - sparse_categorical_crossentropy: 9.4659\n",
            "Epoch 19/40\n",
            "168/168 [==============================] - 8s 45ms/step - loss: 9.4439 - sparse_categorical_crossentropy: 9.4662\n",
            "Epoch 20/40\n",
            "168/168 [==============================] - 8s 46ms/step - loss: 9.4439 - sparse_categorical_crossentropy: 9.4660\n",
            "Epoch 21/40\n",
            "168/168 [==============================] - 8s 45ms/step - loss: 9.4439 - sparse_categorical_crossentropy: 9.4659\n",
            "Epoch 22/40\n",
            "168/168 [==============================] - 8s 45ms/step - loss: 9.4439 - sparse_categorical_crossentropy: 9.4651\n",
            "Epoch 23/40\n",
            "168/168 [==============================] - 8s 46ms/step - loss: 9.4439 - sparse_categorical_crossentropy: 9.4638\n",
            "Epoch 24/40\n",
            "168/168 [==============================] - 8s 45ms/step - loss: 9.4439 - sparse_categorical_crossentropy: 9.4641\n",
            "Epoch 25/40\n",
            "168/168 [==============================] - 8s 46ms/step - loss: 9.4439 - sparse_categorical_crossentropy: 9.4638\n",
            "Epoch 26/40\n",
            "168/168 [==============================] - 8s 46ms/step - loss: 9.4439 - sparse_categorical_crossentropy: 9.4647\n",
            "Epoch 27/40\n",
            "168/168 [==============================] - 8s 46ms/step - loss: 9.4439 - sparse_categorical_crossentropy: 9.4654\n",
            "Epoch 28/40\n",
            "168/168 [==============================] - 8s 46ms/step - loss: 9.4439 - sparse_categorical_crossentropy: 9.4655\n",
            "Epoch 29/40\n",
            "168/168 [==============================] - 8s 48ms/step - loss: 9.4524 - sparse_categorical_crossentropy: 9.4689\n",
            "Epoch 30/40\n",
            "168/168 [==============================] - 8s 45ms/step - loss: 9.4444 - sparse_categorical_crossentropy: 9.5133\n",
            "Epoch 31/40\n",
            "168/168 [==============================] - 8s 47ms/step - loss: 9.4441 - sparse_categorical_crossentropy: 9.5155\n",
            "Epoch 32/40\n",
            "168/168 [==============================] - 8s 46ms/step - loss: 9.4441 - sparse_categorical_crossentropy: 9.5156\n",
            "Epoch 33/40\n",
            "168/168 [==============================] - 8s 46ms/step - loss: 9.4441 - sparse_categorical_crossentropy: 9.5157\n",
            "Epoch 34/40\n",
            "168/168 [==============================] - 8s 47ms/step - loss: 9.4441 - sparse_categorical_crossentropy: 9.5156\n",
            "Epoch 35/40\n",
            "168/168 [==============================] - 8s 46ms/step - loss: 9.4441 - sparse_categorical_crossentropy: 9.5157\n",
            "Epoch 36/40\n",
            "168/168 [==============================] - 8s 46ms/step - loss: 9.4441 - sparse_categorical_crossentropy: 9.5156\n",
            "Epoch 37/40\n",
            "168/168 [==============================] - 8s 46ms/step - loss: 9.4441 - sparse_categorical_crossentropy: 9.5156\n",
            "Epoch 38/40\n",
            "168/168 [==============================] - 8s 46ms/step - loss: 9.4441 - sparse_categorical_crossentropy: 9.5157\n",
            "Epoch 39/40\n",
            "168/168 [==============================] - 8s 46ms/step - loss: 9.4441 - sparse_categorical_crossentropy: 9.5156\n",
            "Epoch 40/40\n",
            "168/168 [==============================] - 8s 47ms/step - loss: 9.4441 - sparse_categorical_crossentropy: 9.5156\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(seed_text, next_words, model, max_sequence_len, temperature=1.0):\n",
        "    for _ in range(next_words):\n",
        "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "        token_list = pad_sequences([token_list], maxlen=max_sequence_len - 1, padding='pre')\n",
        "        predictions = model.predict(token_list, verbose=0)[0]\n",
        "\n",
        "        predictions = predictions / np.sum(predictions)\n",
        "        predictions = np.log(predictions + 1e-8) / temperature\n",
        "        predictions = np.exp(predictions) / np.sum(np.exp(predictions))\n",
        "\n",
        "        try:\n",
        "            predicted_word_index = np.random.choice(range(total_words), p=predictions.ravel())\n",
        "        except ValueError:\n",
        "            print(\"Error: Probabilidades contienen NaN o no suman 1.\")\n",
        "            break\n",
        "\n",
        "        output_word = tokenizer.index_word.get(predicted_word_index, \"\")\n",
        "        seed_text += \" \" + output_word\n",
        "\n",
        "    return seed_text\n",
        "\n",
        "print(generate_text(\"THE KING: \", next_words=100, model=model, max_sequence_len=max_sequence_len, temperature=1.0))"
      ],
      "metadata": {
        "id": "2QSLgJQqXL1G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "910630ca-7731-4638-ab7d-bfb8786697be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "THE KING:  bale free afore swashing justice greece requites caparison lewd lour signior malmsey flowed awakens aeneas' strait undergoing tripp'd pass'd mattock jealous arriving streams beguil'd shoemaker counterfeiting bristol solely heal'd express'd satisfaction achieve ghosts liquor until stinkingly darken minds companion remorseless westminster agents music action modern cleomenes shot chance kiss intends madman's mothers microcosm visitation sir' narrow pomp outdone kiln stumbling seducing kindness doublet swooned soaking morrows fee'd seized brandish soil beloved insufficience coloured screen woodman heretics thrives fill'd roses methought trenchers fleece 'romeo through't surges gainsays shape craftily cracks seld commandments cony approach shelves wittingly cheat arbitrement sermon terror mouth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate_text(\"THE KING: \", next_words=100, model=model, max_sequence_len=max_sequence_len, temperature=10.0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZPnO0RnDKhWn",
        "outputId": "64c74ac0-0f72-4394-8a7c-f1003a80033c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "THE KING:  honourest mistook scale fawns termed required requital purpose frosts awaked spokest pleased wherefore sovereign's beneath fares haply dishonours now deformity let'st doxy finger'd shoot entertain'st pertain presenteth snatch'd bight bravery moated fresh privately bared scorn'd repined seat retail numbness surfeiting husband lane villages nest 'deny lest influence colts page keeps stream frets hastening bub stranger power undeserved stays number'd prefixed consummate revel enfranchised censures searchers drizzle felony highmost rung hostile fingering unity 'citizens erpingham tinker's strangers even burnt crone gabble sorrowing strumpet curdied scatter'd hinds powder severing shade bolted quality sluggard brawling unfolding barnardine's liest impatient brocas mount surer unprofitable\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate_text(\"THE KING: \", next_words=100, model=model, max_sequence_len=max_sequence_len, temperature=0.1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OnQ2tJouKkPO",
        "outputId": "d69fbab5-967a-4e8b-837f-07ec59abfc5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "THE KING:  shave substitutes permitted vilely substitutes it shameful volume gifts unblown comedy graces what graces gifts shameful is is shameful shameful is shameful maim'd desperation aufidius comedy it it shameful is unblown aufidius graces shameful eden vilely vilely is blemish vilely vassal shameful shave shameful graces aufidius dire shameful gifts substitutes comedy is faces comedy shave is gild shave perceived eden richard to heirless shameful shameful dire shameful shave eden eden eden a richard vilely raze aufidius is gild richard shameful shameful gild faces unblown raze recovered substitutes raze shameful shameful comedy heirless volume shameful shameful shameful dire gifts shameful vassal\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusión temperatura**\n",
        "\n",
        "Temperatura = 1: El modelo generó texto con un estilo coherente y palabras con  relación al texto original.\n",
        "\n",
        "Temperatura = 10: Con una temperatura alta, el modelo generó combinaciones de palabras que no tienen estructura de idioma ni el estilo.\n",
        "\n",
        "Temperatura = 0.1: Con temperaturas cercanas a cero, el modelo comenzó a repetir patrones textuales o combinaciones similares a las del texto original.\n"
      ],
      "metadata": {
        "id": "wnTcwE0EozU1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate_text(\"THE KING: \", next_words=10, model=model, max_sequence_len=max_sequence_len, temperature=1.0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lwAHJuXugGRQ",
        "outputId": "5f33fc1a-10d5-4437-e9a2-37879f7a15ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "THE KING:  call't devilish 'thus accoutrements alarum outcast procures substance shamefast stewardship\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate_text(\"THE KING: \", next_words=25, model=model, max_sequence_len=max_sequence_len, temperature=1.0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hcfg6_NmgHcw",
        "outputId": "bba5871e-c2c5-44d7-f72e-6f443afffe12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "THE KING:  wearied righteous villain idles widowhood singing wistly bulks censer oratory successor events commit forbiddenly bridge soldiers' considering barbary defence discipline credulous shoulders gramercies in amort\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate_text(\"THE KING: \", next_words=50, model=model, max_sequence_len=max_sequence_len, temperature=1.0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dDC7j4djgD0Z",
        "outputId": "a9650982-7310-4f69-841d-c7ebaf65d55d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "THE KING:  changing unthankfulness puff'd medlars 'not dumbly spiritual dame instant descending drivelling meets small fitness cruel tottering liege derived 'banished' prate hover fares italian disproved sob levy wish't unbegot suspicious 'good pick a blue discredited tempt changed devices prospero uncrown hardly ungently tempting pretences flaw'd scent eve governs contending implore flattering\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate_text(\"THE KING: \", next_words=100, model=model, max_sequence_len=max_sequence_len, temperature=1.0))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VaFfllHugJ2O",
        "outputId": "fd2dacd7-f5eb-4375-ed45-2cccf52fff1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "THE KING:  punish'd lamentation sibyl dash'd tarquin bestow barely unapt aumerle powdered notedly chamberlain distaffs women's divines limp volume carousing goal excuses children's modesties requireth move permissive 'the neat counterpoised owed success casements imprison cominius callat demerits effect crack uncurse blemish titles howe'er urine hyrcania precepts steel'd minds misleading trooping voluptuously budger divideth lop often abbey lack'st poisoner denying walter's dog orderly where'er graft noblesse coffins urgent disdainful reprieve valentine unlucky mystery tradition repliest writ unclasp'd needle coop'd admiral cured inquiry dealing interrupts bliss succeeders listen woe's guiding royally frost imagine smilest misusest vouches interrupted you' nations fewness graze placed rarest levy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate_text(\"THE KING: \", next_words=200, model=model, max_sequence_len=max_sequence_len, temperature=1.0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ym0JzP0gfA9",
        "outputId": "06d0de4b-bd2e-4abf-ca24-cd1c898cba3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "THE KING:  ungovern'd thimble luke's delectable hermitage busy assist fornication either's unarm'd linger confident debatement peer'd dishonesty unclog cheap elbow's distill'd singly inquire elizabeth unstaid wildly forenamed great'st minds offering trumpeter trinkets speaking pear shepherd herring touch'd fees vouch'd flocks waiteth tumbled wisest associated flecked pithy riding traces interr'd wedged wean'd strucken psalteries seedness wing'd 'cucullus know'st shakest scent institutions strangeness unwieldy misproud ay obey'd singer conditionally wills sings pupil killer appeared corrupt where disorder'd official comeliness refresh plain sleekly empiricutic seeing makes yourselves rail'st receive eyeball 'faith laying cooling likely relished 'whoop montgomery oppresseth insolent premeditation maintained crow'd tybalts fitness brutus plough lockram jaws truer it sell advantaged spun angels minos harm blubbering senis mightiest found help pantry invocate waterton wagon sluggard claudio mock'd monstrous weary boy's blessing flaws worms confessed deafening ships purposes them square containing dignifies knocks pardoning utmost 'jump feet disproportion instinctively england well sharps devil years ladder pe truth talk profane unknown biting city's remorse slanderers bodied gorgeous darest thrive absolver women's relished colours strive lightning countenance clutch'd curb swain lulls doubleness throwing coat counterpoise treason bridal withhold flowered theme briers owed brand exiled jugs novice bepaint forswear fry roundly requited elder upon's magnificence wooes disdainful snarling\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusión longitud:**\n",
        "\n",
        "*Secuencia de 10 palabras:* El modelo genera palabras aisladas que no forman frases completas ni coherentes.\n",
        "\n",
        "*Secuencia de 25 palabras: *Las palabras comienzan a conectarse con coherencia dentro del estilo shakesperiano.\n",
        "\n",
        "*Secuencia de 50 palabras:* Se observa una mejora en la estructura de las frases. Aunque las ideas no están desarrolladas por completo.\n",
        "\n",
        "*Secuencia de 100 palabras:* El texto generado presenta una mayor coherencia dentro del estilo shakesperiano. Las frases son más largas y complejas.\n",
        "\n",
        "*Secuencia de 200 palabras:* Aparecen frases desconectadas o redundantes.\n"
      ],
      "metadata": {
        "id": "TzMJVglshWbc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusión"
      ],
      "metadata": {
        "id": "yuG12sHtiAVl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Entendemos que la mejor combinación entre longitud y temperatura para tener un buen rendimiento y que la generación de texto sea coherente es temperatura = 1, longitud de secuencia = 100.\n",
        "\n",
        "*Inglés:*\n",
        "\n",
        "THE KING:  punish'd lamentation sibyl dash'd tarquin bestow barely unapt aumerle powdered notedly chamberlain distaffs women's divines limp volume carousing goal excuses children's modesties requireth move permissive 'the neat counterpoised owed success casements imprison cominius callat demerits effect crack uncurse blemish titles howe'er urine hyrcania precepts steel'd minds misleading trooping voluptuously budger divideth lop often abbey lack'st poisoner denying walter's dog orderly where'er graft noblesse coffins urgent disdainful reprieve valentine unlucky mystery tradition repliest writ unclasp'd needle coop'd admiral cured inquiry dealing interrupts bliss succeeders listen woe's guiding royally frost imagine smilest misusest vouches interrupted you' nations fewness graze placed rarest levy\n",
        "\n",
        "*Español:*\n",
        "\n",
        "EL REY: lamento castigado sibila guionado tarquino otorgar apenas inapto aumerle empolvado notablemente chambelán ruecas teólogos de las mujeres volumen cojo juerga objetivo excusas modestias de los niños requiere movimiento permisivo 'el pulcro contrapeso éxito debido ventanas encarcelar cominius callat deméritos efecto crack sin maldición mancha títulos howe' er orina hircania preceptos mentes aceradas tropas engañosas voluptuoso periquito divide lop a menudo abadía falta envenenador negando el perro de walter ordenanza dónde injerto nobleza ataúdes urgente desdeñoso indulto San Valentín desafortunado misterio tradición respuesta orden judicial desabrochada aguja cooperativa almirante curado investigación trato interrumpe felicidad sucesores escucha aflicción guiando real escarcha imagina más sonriente Los vales mal utilizados interrumpieron la escasez de naciones y el impuesto más raro."
      ],
      "metadata": {
        "id": "PxnoYds4iDI-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CONCLUSIÓN GENERAL"
      ],
      "metadata": {
        "id": "FLCSZuYriYfG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Carácter a carácter:**\n",
        "\n",
        "*   El texto generado presenta una estructura similar al estilo shakesperiano en términos de forma, con palabras consistentes con el texto. Sin embargo, crea palabras inexistentes, como \"hustany\" o \"anner\".\n",
        "*   La coherencia general es limitada, con frases sin un significado claro. Por ejemplo, \"Me alegro de que alguna vez pudieras haberlo visto, y cada uno muera en silencio\".\n",
        "*   Este enfoque es más propenso a introducir errores en la formación de palabras, pero logra replicar el \"estilo visual\" del texto shakesperiano.\n",
        "\n",
        "**Modelo palabra a palabra:**\n",
        "\n",
        "*   El texto muestra mayor coherencia semántica, con palabras reales. Sin embargo, algunas frases parecen forzadas, como \"títulos manchados sin hircania urinaria\".\n",
        "*   Las frases son más largas y estructuradas, aunque a veces pierde el contexto.\n",
        "\n",
        "**Comparación:**\n",
        "\n",
        "*   El modelo carácter a carácter genera términos nuevos. Por su parte, el modelo palabra a palabra se centra más en el texto, limitando la creación de términos nuevos pero mejorando la coherencia.\n",
        "*   El modelo palabra a palabra supera al de carácter a carácter en coherencia, logrando frases con más sentido y estructura.\n",
        "*   Ambos enfoques capturan el estilo shakesperiano, el primero lo refleja más en la forma, mientras que el segundo más en el contenido.\n"
      ],
      "metadata": {
        "id": "2DkfHFv4ichq"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}